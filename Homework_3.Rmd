---
title: "Homework_3"
author: "Donninelli, Cusma Fait, Behrouz, El gataa"
date: "2023-12-03"
output: html_document
---

```{r, echo=FALSE}
set.seed(0)
```



## FSDS - Chapter 6

### Ex 6.12

For the ```UN``` data file at the book’s website (see Exercise 1.24), construct a multiple regression model predicting Internet using all the other variables. Use the concept of multicollinearity to explain why adjusted $R^2$ is not dramatically greater than when GDP is the sole predictor. Compare the estimated GDP effect in the bivariate model and the multiple regression model and explain why it is so much weaker in the multiple regression model.

**Solution**


```{r 6_12, echo=TRUE}
UN.data <- read.table("https://stat4ds.rwth-aachen.de/data/UN.dat", header=TRUE)

# We construct a multiple linear regression as requested, using all variables to predict "internet"
m_full <- with(UN.data, lm(Internet ~ . - Nation, data=UN.data))  # Concise way to say "all but Nation"
summary(m_full)

# We now build the reduced model which only uses GDP as predictor
m_gdp <- with(UN.data, lm(Internet ~ GDP, data=UN.data))
summary(m_gdp)
```
As evident from the comparison, the full model yields an adjusted $R^2$ value of $0.8164$, while the GDP-only model shows a value of $0.7637$.
Notably, examining the p-value associated with the GDP predictor's t-test reveals a significantly stronger impact in the GDP-only model than indicated in the full model.

These disparities often arise due to multicollinearity among predictors, multicollinearity is a common problem for regression modelling and occurs when some of our predictors are linearly interrelated. Multicollinearity can deflate the adjusted $R^2$ value, as the penalty for adding predictors outweighs the information gain. In case of multicollinearity a single predictor can also have a significance value which does not reflect its true importance. This occurs because the predictor nearly replicates a linear combination of others, impacting its perceived importance in the model.

We can use the variance inflation factor (VIF) to assess multicollinearity impact. VIF measures the effect of correlation with other variables in increasing the standard error of a regression coefficient. As a rule of thumb, we can detect an high multicollinearity if $VIF(\beta_j) > 10$.
```{r 6_12bis, echo=TRUE}
library(car)
vif(m_full)
```
As we can see there's an high chance that multicollinearity is the cause of the behaviour we observed.



### Ex 6.14

The data set ```Crabs2``` at the book’s website comes from a study of factors that affect sperm traits of male horseshoe crabs. A response variable, $SpermTotal$, is the log of the total number of sperm in an ejaculate. It has $\bar y = 19.3$ and $s = 2.0$x. The two explanatory variables used in the ```R``` output are the horseshoe crab’s $carapace width$ (CW, mean 18.6 cm, standard deviation 3.0 cm), which is a measure of its size, and $color$ (1 = dark, 2 = medium, 3 = light), which is a measure of adult age, darker ones being older.

**Solution**

(a) Prediction Equation and Parameter Interpretation:

The regression model can be expressed as:

```
SpermTotal=11.366+0.391*CW+0.809*factor(Color)2+1.149*factor(Color)3

    Intercept (11.366): Represents the baseline log of total sperm count when both carapace width (CW) and color are zero (or for the reference color).

    CW (0.391): Implies that with each unit increase in carapace width, we anticipate a 0.391 unit increase in the log of total sperm count, holding color constant.

    factor(Color)2 (0.809): Indicates the expected difference in log sperm count between color category 2 and the reference category (likely color 1).

    factor(Color)3 (1.149): Represents the expected difference in log sperm count between color category 3 and the reference category.
```

(b) Explanation of F Statistics:

(i) For the Overall Model:
  
```
    F value: $55.06$
    p-value: $< 2.2e-16$
```

The combined impact of CW, factor(Color), and their interaction significantly influences the log of total sperm count. At least one of these predictors contributes significantly to the model, we can further study the relations and see which one contributes.

(ii) For the factor(Color) Effect:

```
    F value: $9.5757$
    p-value: $9.812e-05$
``` 

There are noteworthy differences in log sperm count among various color categories. 
The Color effect is statistically significant.

(iii) For the Interaction Term (CW:factor(Color)):
```
    F value: 1.5198
    p-value: 0.2207
```    
The interaction effect between carapace width (CW) and color is not statistically significant. This implies that the joint impact of CW and color on log sperm count does not deviate significantly from their individual effects.

### Ex 6.30

When the values of $y$ are multiplied by a constant $c$, from their formulas, show that $s_y$ and $\hat\beta_1$ in the bivariate linear model are also then multiplied by c. Thus, show that $r = \hat \beta_1(s_x/s_y)$ does not depend on the units of measurement.

**Solution**

$$
y_i = \beta_0 + \beta_1x_i + \varepsilon_i
$$

If we multiply all values of \(y_i\) by a constant \(c\):

$$
cy_i = c(\beta_0 + \beta_1x_i + \varepsilon_i)
$$
$$
cy_i = c\beta_0 + c\beta_1x_i + c\varepsilon_i
$$

The standard deviation \(s_y\) is given by:
$$
s_y =  \sqrt{\frac{\sum_{i=1}^{n}(cy_i-c\bar{y})^2}{n-1}}
$$
$$
s_y =  \sqrt{\frac{\sum_{i=1}^{n}(c(\beta_0 + \beta_1x_i + \varepsilon_i) - c\bar{y})^2}{n-1}}
$$
$$
s_y =  \sqrt{\frac{\sum_{i=1}^{n}(c\beta_0 + c\beta_1x_i + c\varepsilon_i - c\bar{y})^2}{n-1}}
$$
$$
s_y =  \sqrt{\frac{c^2\sum_{i=1}^{n}(\beta_0 + \beta_1x_i + \varepsilon_i - \bar{y})^2}{n-1}}
$$
$$
s_y = c \sqrt{\frac{\sum_{i=1}^{n}(\beta_0 + \beta_1x_i + \varepsilon_i - \bar{y})^2}{n-1}}
$$
So we see that when we multiply $y$ in a constant $c$ then the $sy$ is also multiplied in this constant.The least squares estimator for \(\beta_1\), denoted as \(\hat{\beta}_1\), remains the same when multiplying \(y\) by a constant \(c\). This is because the least squares estimator is obtained by minimizing the sum of squared differences, and multiplying all values by a constant does not affect the minimization problem.

The correlation coefficient \(r\) between \(x\) and \(y\) is given by:

$$
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
$$
we multiply $y_i$ by a constant $c$
$$
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(c(y_i - \bar{y}))}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(c(y_i - \bar{y}))^2}}
$$
$$
r = \frac{c\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{c^2\sum_{i=1}^{n}(y_i - \bar{y})^2}}
$$
$$
r = \frac{c\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{c\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
$$
So $c$ cancels out in the numerator and denominator and as we show, \(r\) does not depend on the units of measurement.


### Ex 6.42

You can fit the quadratic equation $E(Y) = \beta_0+\beta_1 x+\beta_2 x^2$ by fitting a multiple regression model with $x_1 = x$ and $x_2 = x^2$.

(a)
Simulate 100 independent observations from the model $Y = 40.0−5.0x+0.5x^2+\epsilon$, where $X$ has a uniform distribution over $[0, 10]$ and $\epsilon ∼ N(0, 1)$. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship.

(b) 
Find the correlation between $x$ and $y$ and explain why it is so weak even though the plot shows a strong relationship with a large $R^2$ value for the quadratic model.

**Solution**

a)
```{r 6.42, echo=TRUE}
 set.seed(123)
  n <- 100
  x <- runif(n, 0, 10)
  epsilon <- rnorm(n, 0, 1)
  y <- 40.0 - 5.0 * x + 0.5 * x^2 + epsilon
  data <- data.frame(x = x, x2 = x^2, y = y)
  model <- lm(y ~ x + x2, data = data)
  summary(model)
  plot(x, y, main = "Quadratic Regression", xlab = "x", ylab = "y")
  lines(sort(x), fitted(model)[order(x)], col = "red", lw = 2)
  legend("topright", legend = "Fitted Quadratic Model", col = "red", lty = 1)
  true_relationship <- function(x) {40.0 - 5.0 * x + 0.5 * x^2}
  lines(sort(x), true_relationship(sort(x)), col = "blue", lw = 2)
  legend("topleft", legend = "True Relationship", col = "blue", lty = 1)

  correlation_xy <- cor(x, y)
  r_squared <- summary(model)$r.squared
  cat("R-squared value for the quadratic model:", r_squared, "\n") 
```
b)Because the correlation coefficient measures linear relationships, If the relationship is nonlinear, the correlation may not capture its strength effectively. $R^2$value represents the proportion of variance explained by the model. It can be high even for nonlinear relationships, as long as the model fits the data well.
The weak correlation between $x$ and $y$ doesn't necessarily contradict the large $R^2$ value. The quadratic model is capturing the nonlinear relationship effectively, leading to a high $R^2$value while the correlation reflects the linear association, which may be weak because of the nature of the quadratic.


### Ex 6.52

$F$ statistics have alternate expressions in terms of $R^2$ values.

(a)
Show that for testing $H_0: \beta_1 = ⋯ = \beta_p = 0$,
$$
F = \frac{(T SS − SSE)/p}{SSE/[n − (p + 1)]}
$$
is equivalently 
$$
\frac{R^2/p}{(1 − R^2)/[n − (p + 1)]}
$$
Explain why larger values of $R^2$ yield larger values of $F$.

(b)
Show that for comparing nested linear models,
$$
F = \frac{(SSE_0 − SSE_1)/(p1 − p0)}{SSE_1/[n − (p_1 + 1)]}
=\frac{(R_1^2 − R_0^2)/(p_1 − p_0)}{(1 − R_1^2)/[n − (p_1 + 1)]}
$$

**Solution**

First of all let's recall the definition of $R^2$ and a simple decomposition which is always valid if the intercept is included in the model:
$$
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{TSS}
$$
It follows immediately that
$$
1 - R^2 = \frac{SSE}{TSS}
$$

(a) To derive the result we simply need to divide by $TSS$ both numerator and denominator and apply the previous observation.
$$
\frac{(TSS − SSE)/p}{SSE/[n − (p + 1)]} = \frac{\frac{TSS − SSE}{TSS}}{\frac{SSE}{TSS}} \frac{1/p}{1/[n-(p+1)]} = \frac{1 - \frac{SSE}{TSS}}{1 - R^2 }\frac{1/p}{1/[n-(p+1)]} = \frac{R^2/p}{(1 − R^2)/[n − (p + 1)]}
$$
From this equivalence we can observe that when $R^2$ increases the numerator increases and the denominator decreases, leading to a general increase of $F$.
This makes perfect sense, an increase in $R^2$ generally means that the model constructed is better and $F$ statistic is used to assess whether the used model is better than the null one, therefore it is sensible that when the model get's better the $F$ statistic increases and *the chances of rejecting* the null hypothesis $H_0: \mu_i = \beta_0$ increase.

(b) Assuming to have two nested models we start by observing that the $TSS$ quantity does not depend on the model chosen but only on the data. To obtain the requested result we simply need to divide by $TSS$ both numerator and denominator and apply the observation obtained for (a).
$$
\begin{split}
\frac{(SSE_0 − SSE_1)/(p1 − p0)}{SSE_1/[n − (p_1 + 1)]} &= \frac{\frac{SSE_0 − SSE_1}{TSS}}{\frac{SSE_1}{TSS}} \frac{1/(p1 − p0)}{1/[n − (p_1 + 1)]} =\\
& = \frac{\frac{SSE_0}{TSS} − \frac{SSE_1}{TSS}}{1 - R_1^2} \frac{1/(p1 − p0)}{1/[n − (p_1 + 1)]} =\\
& = \frac{(1 - R_0^2) - (1 - R_1^2)}{1 - R_1^2} \frac{1/(p1 − p0)}{1/[n − (p_1 + 1)]} =\\
&= \frac{(R_1^2 − R_0^2)/(p_1 − p_0)}{(1 − R_1^2)/[n − (p_1 + 1)]}
\end{split}
$$



## FSDS - Chapter 7


### Ex 7.4

Analogously to the previous exercise, randomly sample 30 $X$ observations from a uniform in the interval $(-4,4)$ and conditional on $X = x$, 30 normal observations with $E(Y) = 3.5x^3 − 20x^2 + 0.5x + 20$ and $\sigma = 30$. Fit polynomial normal GLMs of lower and higher order than that of the true relationship. Which model would you suggest? Repeat the same task for $E(Y) = 0.5 x^3 − 20x^2 + 0.5x + 20$ (same $\sigma$) several times. What do you observe? Which model would you suggest now?

**Solution**



```{r 7_4_a}
set.seed(0)

## randomly sampling 30 X observations from a uniform in the interval (−4,4)
n <- 30
x <- runif(n, min=-4, max=4)

## ...and conditional on X=x, 30 normal observations with the given E(Y) and  σ
true_f <- function(x) {
  3.5 * x^3 - 20 * x^2 + 0.5 * x + 20
}

noisy_f <- function(x) {
  true_f(x) + rnorm(1, 0, 30) 
}

# Plotting data and fitting with the true mean value for each x
y <- unlist(lapply(x, noisy_f))
x.thick <- seq(-5, 5, by=.01)
y.fit <- lapply(x.thick, true_f)
plot(x, y, main='Sample A', xlim=c(-4.5,4.5), ylim=c(-600, 150))
lines(x.thick, y.fit, col='red', lty = 'solid')
legend(x = "topleft", legend = c('E(Y)'), 
       lty = c(1), col = c('red'), 
       lwd = 2, bg='transparent', box.col='transparent', cex=0.8)
```



```{r 7_4_b}

## Fitting polynomial normal GLMs of order 2 and 4 
x_2 <- x^2
x_3 <- x^3
x_4 <- x^4

data_4 <- data.frame(x, x_2, x_3, x_4, y)
data_2 <- data.frame(x, x_2, y)

fit_4 <- glm(y ~ x + x_2 + x_3 + x_4, 
             family=gaussian, 
             data=data_4)

fit_2 <- glm(y ~ x + x_2, 
             family=gaussian, 
             data=data_2)

c0 <- coef(fit_4)[1]
c1 <- coef(fit_4)[2]
c2 <- coef(fit_4)[3]
c3 <- coef(fit_4)[4]
c4 <- coef(fit_4)[5]
y_fit_4 <- c0 + c1 * x.thick + c2 * x.thick^2 + c3 * x.thick^3 + c4 * x.thick^4

c0 <- coef(fit_2)[1]
c1 <- coef(fit_2)[2]
c2 <- coef(fit_2)[3]
y_fit_2 <- c0 + c1 * x.thick + c2 * x.thick^2

## Plotting the result of the fits for sample A
plot(x, y, main='Fits for sample A', xlim=c(-4.5,4.5), ylim=c(-650, 200))
lines(x.thick, y.fit, col='red', lty = 'solid')
lines(x.thick, y_fit_4, col='orange', lty = 'dashed')
lines(x.thick, y_fit_2, col='blue', lty = 'dashed')

legend(x = "topleft", legend = c('E(Y)', 'order 4', 'order 2'), 
       lty = c(1, 2, 2), col = c('red', 'orange', 'blue'), 
       lwd = 2, bg='transparent', box.col='transparent', cex=0.8)        


```

The more complex model captures very well the pattern that the sample exhibits, as can be seen by the fact that it closely resembles the true expectation value. The simplest model fails to do so, and this is why, in this case, we should in principle prefer the more complex model, but further testing is recommended.


```{r 7_4_c}

## Repeating the same task for E(Y)=0.5x^3−20x^2+0.5x+20 (same σ)

true_f <- function(x) {
  0.5 * x^3 - 20 * x^2 + 0.5 * x + 20
}

noisy_f <- function(x) {
  true_f(x) + rnorm(1, 0, 30) 
}

# plot data and fit
y <- unlist(lapply(x, noisy_f))
x.thick <- seq(-5, 5, by=.01)
y.fit <- lapply(x.thick, true_f)
plot(x, y, main='Sample B', xlim=c(-4.5,4.5), ylim=c(-600, 150))
lines(x.thick, y.fit, col='red', lty = 'solid')
legend(x = "topleft", legend = c('E(Y)'), 
       lty = c(1), col = c('red'), 
       lwd = 2, bg='transparent', box.col='transparent', cex=0.8)
```



```{r 7_4_d}

# fit
data_4 <- data.frame(x, x_2, x_3, x_4, y)
data_2 <- data.frame(x, x_2, y)

fit_4 <- glm(y ~ x + x_2 + x_3 + x_4, 
             family=gaussian, 
             data=data_4)

fit_2 <- glm(y ~ x + x_2, 
             family=gaussian, 
             data=data_2)


# plot
c0 <- coef(fit_4)[1]
c1 <- coef(fit_4)[2]
c2 <- coef(fit_4)[3]
c3 <- coef(fit_4)[4]
c4 <- coef(fit_4)[5]
y_fit_4 <- c0 + c1 * x.thick + c2 * x.thick^2 + c3 * x.thick^3 + c4 * x.thick^4

c0 <- coef(fit_2)[1]
c1 <- coef(fit_2)[2]
c2 <- coef(fit_2)[3]
y_fit_2 <- c0 + c1 * x.thick + c2 * x.thick^2

## Plotting the result of the fits for sample B
plot(x, y, main='Fits for sample B', xlim=c(-4.5,4.5), ylim=c(-650, 200))
lines(x.thick, y.fit, col='red', lty = 'solid')
lines(x.thick, y_fit_4, col='orange', lty = 'dashed')
lines(x.thick, y_fit_2, col='blue', lty = 'dashed')

legend(x = "topleft", legend = c('E(Y)', 'order 4', 'order 2'), 
       lty = c(1, 2, 2), col = c('red', 'orange', 'blue'), 
       lwd = 2, bg='transparent', box.col='transparent', cex=0.8)        

```

In this case, the third order coefficient barely has any effect on the results of the fit. Therefore, in this case we prefer the simpler model.



### Ex 7.20

In the ```Crabs``` data file introduced in Section 7.4.2, the variable y indicates whether a female horseshoe crab has at least one satellite (1 = yes, 0 = no).

(a) 
Fit a main-effects logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a $95\%$ confidence interval for the weight effect.

(b) 
Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.

(c) 
Use AIC to determine which models seem most sensible among the models with 
(i) interaction, 
(ii) main effects, 
(iii) weight as the sole predictor, 
(iv) color as the sole predictor, and 
(v) the null model.


**Solution**


```{r 7_20_a}
url <- "http://stat4ds.rwth-aachen.de/data/Crabs.dat"
crabs <- read.table(url, header=TRUE)

summary(crabs)
```

```{r 7_20_b}
# Fitting a main-effects logistic model using weight and categorical color as explanatory variables
fit_main <- glm(y ~ weight + factor(color), 
                family='binomial', 
                data=crabs)

summary(fit_main)
```

```{r 7_20_c}
# 95% confidence interval for the weight effect
mu <- coef(fit_main)['weight']
z <- qnorm(0.95)
serr <- summary(fit_main)$coefficients[2, 2]
conf_int <- mu + c(-1,1) * serr * z

# Fitting the model that permits interaction between color as a factor and weight in their effects
fit_inter <- glm(y ~ weight * factor(color), 
                 family='binomial', 
                 data=crabs)

summary(fit_inter)
```

```{r 7_20_d}
# Testing whether this model provides a significantly better fit with anova
anova(fit_main, fit_inter)
```

```{r 7_20_e}
# Here we use AIC to determine which models seem most sensible among the models with: interaction, main effects, weight as the sole predictor, color as the sole predictor, and the null model
fit_weight <- glm(y ~ weight, 
                  family='binomial', 
                  data=crabs)

fit_color <- glm(y ~ factor(color), 
                 family='binomial', 
                 data=crabs)

fit_null <- glm(y ~ 1, 
                family='binomial', 
                data=crabs)

result <- AIC(fit_inter, fit_main, fit_weight, fit_color, fit_null)
result
```

```{r 7_20_f}
plot(result$df, result$AIC, main='fits AIC', xlim=c(0.5, 9.5), ylim=c(190, 240))
text(result$df, result$AIC+6, rownames(result))
```

Given the semantic that a lower AIC score is better, and a lower number of degrees of freedom is also better, the plot suggests that we should in general choose the model that relies on the weight alone.



### Ex 7.26

A headline in $The Gainesville Sun$ (Feb. 17, 2014) proclaimed a worrisome spike in shark attacks in the previous two years. The reported total number of shark attacks in Florida per year from 2001 to 2013 were 33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23. Are these counts consistent with a null Poisson model? Explain, and compare aspects of the Poisson model and negative binomial model fits.


**Solution**


```{r 7_26_a}
library(MASS)

x <- 2001:2013
y <- c(33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23)
plot(x, y, main='Shark attacks', xlab='year', ylab='attacks')

```

```{r 7_26_b}
# Fitting the Poisson model
fit <- glm(y ~ x, family = poisson)
summary(fit)
```

The Poisson model shows a barely significant decline in the shark attacks over time.

```{r 7_26_c}
# Fitting the negative Poisson model
fit_nb <- glm.nb(y ~ x)
summary(fit_nb)
```

```{r 7_26_d}
# Plot
plot(x, y, main="Poisson vs Negative Binomial", xlab='year', ylab='attacks')
lines(x, fitted(fit), col = "red")
lines(x, fitted(fit_nb), col = "blue", lty = 'dashed')

legend(x = "bottomleft", legend = c('Poisson', 'Negative Binomial'), 
       lty = c(1, 2), col = c('red', 'blue'), 
       lwd = 2, bg='transparent', box.col='transparent', cex=0.8)  
```

The negative binomial model doesn't present any significant discrepancy from the Poisson model.




## DAAG - Chapter 8

### Ex 6

As in the previous exercise, the function ```poissonsim()``` allows for experimentation with Poisson regression. In particular, ```poissonsim()``` can be used to simulate Poisson responses with log-rates equal to $a + b \space x$, where $a$ and $b$ are fixed values by default.

(a) Simulate 100 Poisson responses using the model
$$log \space \lambda = 2 − 4x$$
for x = 0, 0.01, 0.02 ..., 1.0. Fit a Poisson regression model to these data, and compare the estimated coefficients with the true coefficients. How well does the estimated model predict future observations?

(b) Simulate 100 Poisson responses using the model
$$log \space \lambda = 2 − b \space x$$
where b is normally distributed with mean 4 and standard deviation 5. [Use the argument ```slope.sd=5``` in the ```poissonsim()``` function.] How do the results using the poisson and ```quasipoisson``` families differ?

**Solution**

```{r}
# Install and load necessary libraries
# install.packages("MASS")
library(MASS)

# Function to simulate Poisson responses
poissonsim <- function(n, x, intercept, slope, slope.sd = 0) {
  if (slope.sd == 0) {
    # Case (a): Fixed slope
    log_lambda <- intercept + slope * x
  } else {
    # Case (b): Random slope with normal distribution
    slope_values <- rnorm(length(x), mean = slope, sd = slope.sd)
    log_lambda <- intercept + slope_values * x
  }
  
  y <- rpois(n, exp(log_lambda))
  
  return(data.frame(x = x, y = y))
}

# (a) Simulate 100 Poisson responses using the model log λ = 2 − 4x
set.seed(123)
x_values_a <- seq(0, 1, by = 0.01)
sim_data_a <- poissonsim(101, x_values_a, intercept = 2, slope = -4)

# Fit Poisson regression model
model_a <- glm(y ~ x, family = poisson(), data = sim_data_a)

# Display true and estimated coefficients
cat("True coefficients: Intercept = 2, Slope = -4\n")
cat("Estimated coefficients:", coef(model_a), "\n")

# Predict future observations
new_data_a <- data.frame(x = seq(0, 1, by = 0.01))
predicted_values_a <- predict(model_a, newdata = new_data_a, type = "response")

# Visualize predictions
plot(sim_data_a$x, sim_data_a$y, col = "blue", pch = 16, main = "Observed vs. Predicted Values (Case a)", xlab = "x", ylab = "y")
lines(new_data_a$x, predicted_values_a, col = "red", lwd = 2)

# (b) Simulate 100 Poisson responses using the model log λ = 2 − bx, where b ~ N(4, 5)
set.seed(456)
x_values_b <- seq(0, 1, by = 0.01)
sim_data_b <- poissonsim(101, x_values_b, intercept = 2, slope = 0, slope.sd = 5)

# Fit Poisson regression model
model_b_poisson <- glm(y ~ x, family = poisson(), data = sim_data_b)

# Fit Quasi-Poisson regression model
model_b_quasipoisson <- glm(y ~ x, family = quasipoisson(), data = sim_data_b)

# Display estimated coefficients for Poisson and Quasi-Poisson models
cat("Poisson Model (Case b) - Estimated coefficients:", coef(model_b_poisson), "\n")
cat("Quasi-Poisson Model (Case b) - Estimated coefficients:", coef(model_b_quasipoisson), "\n")


```

Commentary:

For (a), we simulate data using a fixed slope. After simulating, we fit a Poisson regression model (glm with family poisson) and compare the estimated coefficients with the true coefficients.\
For (b), we simulate data using a normally distributed slope. We fit both a Poisson regression model and a Quasi-Poisson regression model (quasipoisson family) to compare the results. The Quasi-Poisson model is useful when there's overdispersion in the data, and it allows for the estimation of dispersion parameters.

So the Poisson and Quasi-Poisson regression models differ in their treatment of the variance structure. The Quasi-Poisson model is more flexible and can account for overdispersion in the data, where the variance exceeds the mean.
The main difference is how they assume the constant variance.
The Poisson model assume the variance and the mean equal, while the quasi-poisson dont, therefore is better for the inference.
