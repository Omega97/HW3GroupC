---
title: "Homework_3"
author: "Donninelli, Cusma Fait, Behrouz, El gataa"
date: "2023-12-03"
output: html_document
---

```{r, echo=FALSE}
set.seed(0)
```



## FSDS - Chapter 6



### Ex 6.12

For the ```UN``` data file at the book’s website (see Exercise 1.24), construct a multiple regression model predicting Internet using all the other variables. Use the concept of multicollinearity to explain why adjusted $R^2$ is not dramatically greater than when GDP is the sole predictor. Compare the estimated GDP effect in the bivariate model and the multiple regression model and explain why it is so much weaker in the multiple regression model.

**Solution**


```{r 6_12}
UN.data <- read.table("https://stat4ds.rwth-aachen.de/data/UN.dat", header=TRUE)

# We construct a multiple linear regression as requested, using all variables to predict "internet"
m_full <- with(UN.data, lm(Internet ~ . - Nation, data=UN.data))  # Concise way to say "all but Nation"
summary(m_full)

# We now build the reduced model which only uses GDP as predictor
m_gdp <- with(UN.data, lm(Internet ~ GDP, data=UN.data))
summary(m_gdp)
```
As evident from the comparison, the full model yields an adjusted $R^2$ value of $0.8164$, while the GDP-only model shows a value of $0.7637$.
Notably, examining the p-value associated with the GDP predictor's t-test reveals a significantly stronger impact in the GDP-only model than indicated in the full model.

These disparities often arise due to multicollinearity among predictors, multicollinearity is a common problem for regression modelling and occurs when some of our predictors are linearly interrelated. Multicollinearity can deflate the adjusted $R^2$ value, as the penalty for adding predictors outweighs the information gain. In case of multicollinearity a single predictor can also have a significance value which does not reflect its true importance. This occurs because the predictor nearly replicates a linear combination of others, impacting its perceived importance in the model.

We can use the variance inflation factor (VIF) to assess multicollinearity impact. VIF measures the effect of correlation with other variables in increasing the standard error of a regression coefficient. As a rule of thumb, we can detect an high multicollinearity if $VIF(\beta_j) > 10$.
```{r 6_12bis}
library(car)
vif(m_full)
```
As we can see there's an high chance that multicollinearity is the cause of the behaviour we observed.



### Ex 6.14

The data set ```Crabs2``` at the book’s website comes from a study of factors that affect sperm traits of male horseshoe crabs. A response variable, $SpermTotal$, is the log of the total number of sperm in an ejaculate. It has $\bar y = 19.3$ and $s = 2.0$x. The two explanatory variables used in the ```R``` output are the horseshoe crab’s $carapace width$ (CW, mean 18.6 cm, standard deviation 3.0 cm), which is a measure of its size, and $color$ (1 = dark, 2 = medium, 3 = light), which is a measure of adult age, darker ones being older.


**Solution**

...

```{r 6_14}

```

...



### Ex 6.30

When the values of $y$ are multiplied by a constant $c$, from their formulas, show that $s_y$ and $\hat\beta_1$ in the bivariate linear model are also then multiplied by c. Thus, show that $r = \hat \beta_1(s_x/s_y)$ does not depend on the units of measurement.

**Solution**

...

```{r 6_30}

```

...



### Ex 6.42

You can fit the quadratic equation $E(Y) = \beta_0+\beta_1 x+\beta_2 x^2$ by fitting a multiple regression model with $x_1 = x$ and $x_2 = x^2$.

(a)
Simulate 100 independent observations from the model $Y = 40.0−5.0x+0.5x^2+\epsilon$, where $X$ has a uniform distribution over $[0, 10]$ and $\epsilon ∼ N(0, 1)$. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship.

(b) 
Find the correlation between $x$ and $y$ and explain why it is so weak even though the plot shows a strong relationship with a large $R^2$ value for the quadratic model.

**Solution**

...

```{r 6_42}

```

...



### Ex 6.52

$F$ statistics have alternate expressions in terms of $R^2$ values.

(a)
Show that for testing $H_0: \beta_1 = ⋯ = \beta_p = 0$,
$$
F = \frac{(T SS − SSE)/p}{SSE/[n − (p + 1)]}
$$
is equivalently 
$$
\frac{R^2/p}{(1 − R^2)/[n − (p + 1)]}
$$
Explain why larger values of $R^2$ yield larger values of $F$.

(b)
Show that for comparing nested linear models,
$$
F = \frac{(SSE_0 − SSE_1)/(p1 − p0)}{SSE_1/[n − (p_1 + 1)]}
=\frac{(R_1^2 − R_0^2)/(p_1 − p_0)}{(1 − R_1^2)/[n − (p_1 + 1)]}
$$

**Solution**

First of all let's recall the definition of $R^2$ and a simple decomposition which is always valid if the intercept is included in the model:
$$
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{TSS}
$$
It follows immediately that
$$
1 - R^2 = \frac{SSE}{TSS}
$$

(a) To derive the result we simply need to divide by $TSS$ both numerator and denominator and apply the previous observation.
$$
\frac{(TSS − SSE)/p}{SSE/[n − (p + 1)]} = \frac{\frac{TSS − SSE}{TSS}}{\frac{SSE}{TSS}} \frac{1/p}{1/[n-(p+1)]} = \frac{1 - \frac{SSE}{TSS}}{1 - R^2 }\frac{1/p}{1/[n-(p+1)]} = \frac{R^2/p}{(1 − R^2)/[n − (p + 1)]}
$$
From this equivalence we can observe that when $R^2$ increases the numerator increases and the denominator decreases, leading to a general increase of $F$.
This makes perfect sense, an increase in $R^2$ generally means that the model constructed is better and $F$ statistic is used to assess whether the used model is better than the null one, therefore it is sensible that when the model get's better the $F$ statistic increases and *the chances of rejecting* the null hypothesis $H_0: \mu_i = \beta_0$ increase.

(b) Assuming to have two nested models we start by observing that the $TSS$ quantity does not depend on the model chosen but only on the data. To obtain the requested result we simply need to divide by $TSS$ both numerator and denominator and apply the observation obtained for (a).
$$
\begin{split}
\frac{(SSE_0 − SSE_1)/(p1 − p0)}{SSE_1/[n − (p_1 + 1)]} &= \frac{\frac{SSE_0 − SSE_1}{TSS}}{\frac{SSE_1}{TSS}} \frac{1/(p1 − p0)}{1/[n − (p_1 + 1)]} =\\
& = \frac{\frac{SSE_0}{TSS} − \frac{SSE_1}{TSS}}{1 - R_1^2} \frac{1/(p1 − p0)}{1/[n − (p_1 + 1)]} =\\
& = \frac{(1 - R_0^2) - (1 - R_1^2)}{1 - R_1^2} \frac{1/(p1 − p0)}{1/[n − (p_1 + 1)]} =\\
&= \frac{(R_1^2 − R_0^2)/(p_1 − p_0)}{(1 − R_1^2)/[n − (p_1 + 1)]}
\end{split}
$$


## FSDS - Chapter 7



### Ex 7.4

Analogously to the previous exercise, randomly sample 30 $X$ observations from a uniform in the interval $(-4,4)$ and conditional on $X = x$, 30 normal observations with $E(Y) = 3.5x^3 − 20x^2 + 0.5x + 20$ and $\sigma = 30$. Fit polynomial normal GLMs of lower and higher order than that of the true relationship. Which model would you suggest? Repeat the same task for $E(Y) = 0.5 x^3 − 20x^2 + 0.5x + 20$ (same $\sigma$) several times. What do you observe? Which model would you suggest now?

**Solution**

...

```{r 7_4}

```

...



### Ex 7.20

In the ```Crabs``` data file introduced in Section 7.4.2, the variable y indicates whether a female horseshoe crab has at least one satellite (1 = yes, 0 = no).

(a) 
Fit a main-effects logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a $95\%$ confidence interval for the weight effect.

(b) 
Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.

(c) 
Use AIC to determine which models seem most sensible among the models with 
(i) interaction, 
(ii) main effects, 
(iii) weight as the sole predictor, 
(iv) color as the sole predictor, and 
(v) the null model.


**Solution**

...

```{r 7_20}

```

...



### Ex 7.26

A headline in $The Gainesville Sun$ (Feb. 17, 2014) proclaimed a worrisome spike in shark attacks in the previous two years. The reported total number of shark attacks in Florida per year from 2001 to 2013 were 33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23. Are these counts consistent with a null Poisson model? Explain, and compare aspects of the Poisson model and negative binomial model fits.


**Solution**

...

```{r 7_26}

```

...



## DAAG - Chapter 8



### Ex 6

As in the previous exercise, the function ```poissonsim()``` allows for experimentation with Poisson regression. In particular, ```poissonsim()``` can be used to simulate Poisson responses with log-rates equal to $a + b \space x$, where $a$ and $b$ are fixed values by default.

(a) Simulate 100 Poisson responses using the model
$$log \space \lambda = 2 − 4x$$
for x = 0, 0.01, 0.02 ..., 1.0. Fit a Poisson regression model to these data, and compare the estimated coefficients with the true coefficients. How well does the estimated model predict future observations?

(b) Simulate 100 Poisson responses using the model
$$log \space \lambda = 2 − b \space x$$
where b is normally distributed with mean 4 and standard deviation 5. [Use the argument ```slope.sd=5``` in the ```poissonsim()``` function.] How do the results using the poisson and ```quasipoisson``` families differ?

**Solution**

...

```{r 6}

```

...

